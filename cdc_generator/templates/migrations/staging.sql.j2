-- ============================================================================
-- DO NOT EDIT â€” AUTO-GENERATED by: cdc manage-migrations generate
-- Generated: {{ generated_at }}
-- Staging + Merge for: {{ target_schema }}."{{ table_name }}"
-- ============================================================================

-- ============================================================================
-- 1. STAGING TABLE (UNLOGGED for maximum write performance)
-- ============================================================================

CREATE UNLOGGED TABLE IF NOT EXISTS {{ target_schema }}."stg_{{ table_name }}" (
    LIKE {{ target_schema }}."{{ table_name }}" INCLUDING DEFAULTS
) WITH (autovacuum_enabled = false);

-- Add Kafka/Redpanda metadata columns for offset tracking
DO $$
BEGIN
    ALTER TABLE {{ target_schema }}."stg_{{ table_name }}"
        ADD COLUMN IF NOT EXISTS "__kafka_offset" BIGINT,
        ADD COLUMN IF NOT EXISTS "__kafka_partition" INT,
        ADD COLUMN IF NOT EXISTS "__kafka_timestamp" TIMESTAMPTZ;
EXCEPTION WHEN OTHERS THEN
    NULL;
END $$;

-- ============================================================================
-- 2. MERGE STORED PROCEDURE (batched UPSERT, 10k rows per batch)
-- ============================================================================

CREATE OR REPLACE PROCEDURE {{ target_schema }}."sp_merge_{{ table_name | lower }}"()
LANGUAGE plpgsql
AS $$
DECLARE
    v_batch_size INT := 10000;
    v_total_rows BIGINT;
    v_processed  BIGINT := 0;
    v_batch_count INT := 0;
    v_start_time TIMESTAMPTZ;
    v_min_source_ts BIGINT;
    v_max_source_ts BIGINT;
    v_min_offset BIGINT;
    v_max_offset BIGINT;
    v_partition  INT;
BEGIN
    v_start_time := clock_timestamp();

    SELECT
        COUNT(*),
        MIN("__source_ts_ms"::BIGINT),
        MAX("__source_ts_ms"::BIGINT),
        MIN("__kafka_offset"),
        MAX("__kafka_offset"),
        MIN("__kafka_partition")
    INTO
        v_total_rows,
        v_min_source_ts,
        v_max_source_ts,
        v_min_offset,
        v_max_offset,
        v_partition
    FROM {{ target_schema }}."stg_{{ table_name }}";

    IF v_total_rows = 0 THEN
        RAISE NOTICE 'No rows to merge for "{{ table_name }}"';
        RETURN;
    END IF;

    RAISE NOTICE 'Starting merge for "{{ table_name }}": % rows in % batches',
        v_total_rows, CEIL(v_total_rows::NUMERIC / v_batch_size);

    WHILE v_processed < v_total_rows LOOP
        v_batch_count := v_batch_count + 1;

        INSERT INTO {{ target_schema }}."{{ table_name }}" ({{ all_column_names }})
        SELECT DISTINCT ON ({{ pk_column_names }}) {{ all_column_names }}
        FROM {{ target_schema }}."stg_{{ table_name }}"
        ORDER BY {{ pk_column_names }}, "__source_ts_ms" DESC
        LIMIT v_batch_size
        ON CONFLICT ({{ pk_column_names }}) DO UPDATE SET
{{ update_set_sql }};

        DELETE FROM {{ target_schema }}."stg_{{ table_name }}"
        WHERE ctid IN (
            SELECT ctid FROM {{ target_schema }}."stg_{{ table_name }}"
            LIMIT v_batch_size
        );

        v_processed := v_processed + v_batch_size;

        RAISE NOTICE 'Batch %: merged % rows',
            v_batch_count, LEAST(v_batch_size, v_total_rows - v_processed + v_batch_size);
    END LOOP;

    -- Log processing metadata
    BEGIN
        INSERT INTO "cdc_management"."cdc_processing_log" (
            "schema_name", "table_name",
            "min_source_ts_ms", "max_source_ts_ms",
            "min_kafka_offset", "max_kafka_offset", "kafka_partition",
            "records_processed", "merge_duration_ms"
        ) VALUES (
            '{{ target_schema_raw }}', '{{ table_name | lower }}',
            v_min_source_ts, v_max_source_ts,
            v_min_offset, v_max_offset, v_partition,
            v_total_rows,
            EXTRACT(MILLISECONDS FROM (clock_timestamp() - v_start_time))::INT
        );
    EXCEPTION WHEN OTHERS THEN
        RAISE WARNING 'Could not log processing metadata: %', SQLERRM;
    END;

    TRUNCATE {{ target_schema }}."stg_{{ table_name }}";

    RAISE NOTICE 'Merged "{{ table_name }}" from staging: % rows in % batches (%.2f ms)',
        v_total_rows, v_batch_count, EXTRACT(MILLISECONDS FROM (clock_timestamp() - v_start_time));
END;
$$;

-- ============================================================================
-- 3. EVENT-DRIVEN MERGE TRIGGER
-- ============================================================================

DROP TRIGGER IF EXISTS "trg_mark_for_merge" ON {{ target_schema }}."stg_{{ table_name }}";

CREATE TRIGGER "trg_mark_for_merge"
    AFTER INSERT ON {{ target_schema }}."stg_{{ table_name }}"
    FOR EACH STATEMENT
    EXECUTE FUNCTION "cdc_management"."mark_table_for_merge"();

-- ============================================================================
-- 4. PERMISSIONS
-- ============================================================================

GRANT INSERT, SELECT, TRUNCATE ON {{ target_schema }}."stg_{{ table_name }}" TO "{{ db_user }}";
GRANT EXECUTE ON PROCEDURE {{ target_schema }}."sp_merge_{{ table_name | lower }}"() TO "{{ db_user }}";
